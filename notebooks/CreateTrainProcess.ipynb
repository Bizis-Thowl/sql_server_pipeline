{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "c3d05f7d-fb65-4e9f-a566-814f74338ed5",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import math\n",
                "from pandas import DataFrame\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif\n",
                "from datetime import datetime, timedelta, timezone\n",
                "import pyodbc\n",
                "from sqlalchemy import Column, Date, Integer, String, Numeric, create_engine, Float, inspect, func, MetaData, Table, select, ForeignKey\n",
                "from sqlalchemy.ext.declarative import declarative_base\n",
                "from sqlalchemy.orm import mapper, registry, Session, relationship\n",
                "from sqlalchemy.ext.automap import automap_base\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.metrics import f1_score, precision_score, recall_score, mean_squared_error, balanced_accuracy_score \n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.base import BaseEstimator\n",
                "from sklearn.pipeline import Pipeline\n",
                "import scipy\n",
                "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "531a76dc-cb5a-4556-95e6-f783ece904b2",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "#SQLAlchemy Setup\n",
                "def get_engine():\n",
                "    SERVER = 'localhost'\n",
                "    DATABASE = 'metmast'\n",
                "    engine = create_engine(f'mssql+pyodbc:///?odbc_connect=DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={SERVER};DATABASE={DATABASE};Trusted_Connection=yes;TrustServerCertificate=yes;')\n",
                "    return engine\n",
                "\n",
                "engine = get_engine()\n",
                "metadata = MetaData()\n",
                "metadata.reflect(bind=engine)\n",
                "Base = automap_base(metadata=metadata)\n",
                "Base.prepare(autoload_with=engine)\n",
                "mapper_registry = registry()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "datapoints = pd.read_sql_table(\"datapoint\", engine)\n",
                "feature_values_int = pd.read_sql_table(\"datapoint_feature_value_int\", engine)\n",
                "feature_values_float = pd.read_sql_table(\"datapoint_feature_value_float\", engine)\n",
                "features = pd.read_sql_table(\"feature\", engine)\n",
                "features = features[features[\"type_\"]==\"data\"]\n",
                "feature_list = []\n",
                "feature_dict = dict()\n",
                "feature_value_array_dict = dict()\n",
                "\n",
                "for index, row in DataFrame(features).iterrows():\n",
                "    feature = row[\"name\"]\n",
                "    index = row[\"id\"]\n",
                "    feature_dict[index] = feature\n",
                "    feature_list.append(feature)\n",
                "    feature_value_array_dict[feature] = np.empty(datapoints.shape[0], dtype=\"float\")    \n",
                "datapoints = pd.concat([datapoints, pd.DataFrame(columns=feature_list)], axis=1)\n",
                "\n",
                "for index, row in DataFrame(feature_values_int).iterrows():\n",
                "    feature = feature_dict[row[\"feature_id\"]]\n",
                "    row_id = row[\"datapoint_id\"] - 1\n",
                "    feature_value_array_dict[feature][row_id] = row[\"int_\"]\n",
                "for index, row in DataFrame(feature_values_float).iterrows():\n",
                "    feature = feature_dict[int(row[\"feature_id\"])]\n",
                "    row_id = int(row[\"datapoint_id\"]) - 1\n",
                "    feature_value_array_dict[feature][row_id] = row[\"float_\"]\n",
                "    \n",
                "    \n",
                "for feature in feature_list:\n",
                "    datapoints[feature] = feature_value_array_dict[feature]\n",
                "train = datapoints[datapoints['datapoint_mappings_id'] == 1]\n",
                "train.drop(columns=[\"datapoint_mappings_id\"])\n",
                "test = datapoints[datapoints['datapoint_mappings_id'] == 2]\n",
                "test.drop(columns=[\"datapoint_mappings_id\"])\n",
                "print(\"Data loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def update_values(obj, ref_class, session, **kwargs):\n",
                "    #clone = ref_class()\n",
                "    for column in ref_class.__table__.columns:\n",
                "        if(column.name in kwargs):\n",
                "            setattr(obj, column.name, kwargs[column.name])\n",
                "        #else:\n",
                "            #setattr(clone, column.name, getattr(obj, column.name))\n",
                "    #session.delete(obj)\n",
                "    #session.add(clone)\n",
                "    session.commit() \n",
                "    return obj  \n",
                "\n",
                "#Lazy declarations\n",
                "def get_table_by_name(name):\n",
                "    inspector = inspect(engine)\n",
                "    return inspector.get_table(name)\n",
                "\n",
                "def get_feature_id_by_name(Base, name):\n",
                "    query = session.query(metadata.tables[\"feature\"])\n",
                "    df = pd.DataFrame()\n",
                "    for batch in pd.read_sql_query(query.statement, engine, chunksize= 5):\n",
                "        df = pd.concat([df, batch], ignore_index=True)\n",
                "    for index, row in df.iterrows():\n",
                "        if(row[\"name\"]==name):\n",
                "            return row[\"id\"]\n",
                "        \n",
                "def getNextIdForTable(Base, name):\n",
                "    table = Base.classes[name]\n",
                "    return session.query(table.id).count() + 1 \n",
                "  \n",
                "def create_object(Base, class_name, **kwargs):\n",
                "    mapped_class = Base.classes[class_name]\n",
                "    # Create an instance of the mapped class\n",
                "    obj = mapped_class()\n",
                "    # Set the object's attributes\n",
                "    for key, value in kwargs.items():\n",
                "        setattr(obj, key, value)\n",
                "    return obj"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "session = Session(engine)\n",
                "#Get table refrences\n",
                "train_process_table = Base.classes[\"train_process\"]\n",
                "train_process_init_parameter_table = Base.classes[\"train_process_init_parameter\"]\n",
                "#Load initial parameters - use latest parameters if none were explicitly given\n",
                "count_process = session.query(Base.classes.train_process.id).count()\n",
                "count_paras = session.query(Base.classes.train_process_init_parameter.id).count()\n",
                "init_parameters = session.query(train_process_init_parameter_table).order_by(train_process_init_parameter_table.id.desc()).limit(1)[0]\n",
                "if(count_process == count_paras):    \n",
                "    init_parameters_clone = Base.classes.train_process_init_parameter()\n",
                "    for column in Base.classes.train_process_init_parameter.__table__.columns:\n",
                "        if(column.name!=\"id\"):\n",
                "            setattr(init_parameters_clone, column.name, getattr(init_parameters, column.name))\n",
                "        else:\n",
                "            setattr(init_parameters_clone, column.name, getattr(init_parameters, column.name ) + 1)\n",
                "    session.add(init_parameters_clone)\n",
                "    session.commit()\n",
                "#Create meta db Entrys\n",
                "train_process = create_object(Base, \"train_process\", id = count_process + 1)\n",
                "session.add(train_process)\n",
                "session.commit()\n",
                "init_parameters = update_values(init_parameters, Base.classes[\"train_process_init_parameter\"], session, train_process_id = train_process.id )"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
