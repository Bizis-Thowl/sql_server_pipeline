{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import scipy\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from pandas import DataFrame\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (balanced_accuracy_score, f1_score,\n",
    "                             mean_squared_error, precision_score, recall_score)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sqlalchemy import (Column, Date, Float, ForeignKey, Integer, LargeBinary,\n",
    "                        MetaData, Numeric, String, Table, create_engine, func,\n",
    "                        inspect, select)\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import Session, mapper, registry\n",
    "\n",
    "from utils.handle_engine import get_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n",
    "failures_2016 = pd.read_csv(\"../data/init/failures-2016.csv\", sep=\";\")\n",
    "failures_2017 = pd.read_csv(\"../data/init/failures-2017.csv\", sep=\";\")\n",
    "metmast_2016 = pd.read_csv(\"../data/init/metmast-2016.csv\", sep=\";\")\n",
    "metmast_2017 = pd.read_csv(\"../data/init/metmast-2017.csv\", sep=\";\")\n",
    "signals_2016 = pd.read_csv(\"../data/init/signals-2016.csv\", sep=\";\")\n",
    "signals_2017 = pd.read_csv(\"../data/init/signals-2017.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signale beider Jahre kombinieren\n",
    "signals = pd.concat([signals_2016, signals_2017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbine_names = signals[\"Turbine_ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_for_each_turbine(signals):\n",
    "    turbine_dfs = list();\n",
    "\n",
    "    for turbine in turbine_names:\n",
    "        turbine_df = signals[signals[\"Turbine_ID\"] == turbine]\n",
    "        turbine_df = turbine_df.sort_values(\"Timestamp\")\n",
    "        turbine_df = turbine_df.reset_index(drop=True)\n",
    "        turbine_dfs.append(turbine_df)\n",
    "\n",
    "    return turbine_dfs\n",
    "\n",
    "turbine_dfs = create_df_for_each_turbine(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zusammenführen und sortieren\n",
    "metmast = pd.concat([metmast_2016, metmast_2017])\n",
    "metmast = metmast.sort_values(\"Timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop broken met data\n",
    "metmast = metmast.drop([\"Min_Winddirection2\", \"Max_Winddirection2\", \"Avg_Winddirection2\", \"Var_Winddirection2\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill met data\n",
    "metmast = metmast.fillna(method = \"ffill\")\n",
    "metmast = metmast.fillna(method = \"bfill\")\n",
    "metmast.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = pd.concat([failures_2016, failures_2017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mergen\n",
    "def JoinMetamast(df:pd.DataFrame):\n",
    "    df = df.fillna(method = \"ffill\")\n",
    "    df = df.fillna(method = \"bfill\")\n",
    "    df = pd.merge(df, metmast, on=\"Timestamp\", how=\"left\")\n",
    "    df = df.fillna(method = \"ffill\")\n",
    "    df = df.fillna(method = \"bfill\")\n",
    "    df.isna().sum().sum()\n",
    "    return df\n",
    "\n",
    "merged = list()\n",
    "for turbine_df in turbine_dfs:\n",
    "    merged.append(JoinMetamast(turbine_df))\n",
    "merged_df = pd.concat(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures_gearbox = failures[failures[\"Component\"] == \"GEARBOX\"]\n",
    "failures_gearbox.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Util Functions\n",
    "def get_round_minute_diff(datetime_in: datetime) -> timedelta:\n",
    "    min = datetime_in.minute\n",
    "    rounded_min = round(min, -1)\n",
    "    diff = rounded_min - min\n",
    "    return timedelta(minutes=diff)\n",
    "\n",
    "def convert_round_minute_to_time(datetime_in: datetime) -> datetime:\n",
    "    td = get_round_minute_diff(datetime_in)\n",
    "    return datetime_in + td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_lookback = 90\n",
    "mins_per_class = 24 * 60 / 10\n",
    "ten_mins_of_n_days = int(24 * 60 * days_lookback / 10) \n",
    "target_name = \"Class\"\n",
    "\n",
    "def GetClass(i:int)->int:\n",
    "    return math.floor(i/mins_per_class)\n",
    "\n",
    "def create_failure_list() -> pd.DataFrame:\n",
    "    failure_list = []\n",
    "    for i, failure in enumerate(failures_gearbox):\n",
    "        turbine_id = str(failures_gearbox[\"Turbine_ID\"][i])\n",
    "        failure_ts = str(failures_gearbox[\"Timestamp\"][i])\n",
    "        failure_datetime = datetime.fromisoformat(failure_ts)\n",
    "        rounded_datetime = convert_round_minute_to_time(failure_datetime)\n",
    "        for j in range(ten_mins_of_n_days):\n",
    "            delta = timedelta(minutes=j*10)\n",
    "            new_datetime = rounded_datetime - delta\n",
    "            datetime_formated = new_datetime.replace(tzinfo=timezone.utc)\n",
    "            failure_list.append([turbine_id, datetime_formated.isoformat(), GetClass(j)])    \n",
    "    failure_df = pd.DataFrame(failure_list, columns=[\"Turbine_ID\", \"Timestamp\", target_name])\n",
    "    return failure_df\n",
    "\n",
    "failure_df_class  = create_failure_list()\n",
    "#Der Feature-Datensatz wird mit den Labels zusammengeführt. Dabei ist besonders wichtig, dass der Bezug zu der jeweiligen Turbine bestehen bleibt.\n",
    "labeled_df = pd.merge(merged_df, failure_df_class, on=[\"Turbine_ID\", \"Timestamp\"], how=\"left\");\n",
    "labeled_df = labeled_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_failure_list(classes: list[str], days_per_class: int, target_name: str) -> pd.DataFrame:\n",
    "    days_lookback = len(classes) * days_per_class\n",
    "    ten_mins_of_n_days = int(24 * 60 * days_lookback / 10)\n",
    "    failure_list = []\n",
    "    for i, failure in enumerate(failures_gearbox):\n",
    "        turbine_id = str(failures_gearbox[\"Turbine_ID\"][i])\n",
    "        failure_ts = str(failures_gearbox[\"Timestamp\"][i])\n",
    "        failure_datetime = datetime.fromisoformat(failure_ts)\n",
    "        rounded_datetime = convert_round_minute_to_time(failure_datetime)\n",
    "        for iterator, current_class in enumerate(classes):\n",
    "            for j in range(ten_mins_of_n_days):\n",
    "                delta = timedelta(minutes=j*10)\n",
    "                # Prüfen ob obere und untere Schranke passen.\n",
    "                is_in_class = delta >= timedelta(days=iterator*days_per_class) and delta < timedelta(days=(iterator+1) * days_per_class)\n",
    "                if (is_in_class):\n",
    "                    new_datetime = rounded_datetime - delta\n",
    "                    datetime_formated = new_datetime.replace(tzinfo=timezone.utc)\n",
    "                    failure_list.append([turbine_id, datetime_formated.isoformat(), current_class])\n",
    "    \n",
    "    failure_df = pd.DataFrame(failure_list, columns=[\"Turbine_ID\", \"Timestamp\", target_name])\n",
    "\n",
    "    return failure_df\n",
    "\n",
    "class_target_name = \"Risk Level\"\n",
    "risk_levels = [\"low\", \"high\", \"med-high\", \"medium\", \"low-med\"]\n",
    "days_per_class = 18\n",
    "\n",
    "failure_df_multiclass = create_failure_list(classes=risk_levels, days_per_class=days_per_class, target_name=class_target_name)\n",
    "labeled_df = pd.merge(labeled_df, failure_df_multiclass, on=[\"Turbine_ID\", \"Timestamp\"], how=\"left\"); \n",
    "labeled_df = labeled_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df[class_target_name].fillna(\"low\", inplace = True)\n",
    "labeled_df[target_name].fillna(90, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df[target_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Daten ab August 2017 liegen im Testset\n",
    "split_criterion_reg = labeled_df[\"Timestamp\"] >= \"2017-08-00T00:00:00+00:00\"\n",
    "\n",
    "test_gearbox = labeled_df[split_criterion_reg].reset_index(drop=True)#.iloc[:100].reset_index(drop=True)\n",
    "train_gearbox = labeled_df[~split_criterion_reg].reset_index(drop=True)#.iloc[:100].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = get_engine()\n",
    "mapper_registry = registry()\n",
    "\n",
    "label_columns = list([\"Class\", \"Risk Level\"])\n",
    "feature_columns = list(train_gearbox.columns)\n",
    "feature_columns = [x for x in feature_columns if x not in label_columns]\n",
    "meta_columns = list(train_gearbox.select_dtypes(include=['object']).columns)\n",
    "meta_columns = [x for x in meta_columns if x not in label_columns]\n",
    "feature_columns = [x for x in feature_columns if x not in meta_columns]\n",
    "\n",
    "# Define a class to map to the table\n",
    "class Train_data:\n",
    "    pass\n",
    "\n",
    "# Create columns\n",
    "feature_column_types = [Float] * len(feature_columns)\n",
    "f_columns = [Column('id', Integer, primary_key=True, autoincrement=True)] + [\n",
    "    Column(name, type) for name, type in zip(feature_columns, feature_column_types)\n",
    "]\n",
    "\n",
    "# Create table\n",
    "metadata = MetaData()\n",
    "train_data_table = Table('train_data', metadata, *f_columns)\n",
    "metadata.create_all(engine)\n",
    "\n",
    "# Map the class imperatively\n",
    "mapper_registry.map_imperatively(\n",
    "    class_ = Train_data,\n",
    "    local_table = train_data_table\n",
    ")\n",
    "\n",
    "# Define a class to map to the table\n",
    "class Train_data_label:\n",
    "    pass\n",
    "\n",
    "# Create columns\n",
    "label_column_types = list([Integer, String])\n",
    "l_columns = [Column('id', Integer, primary_key=True, autoincrement=True), \n",
    "             Column(\"train_data_id\", Integer, ForeignKey(Train_data.id))] + [\n",
    "    Column(name, type) for name, type in zip(label_columns, label_column_types)    \n",
    "]\n",
    "\n",
    "# Create table\n",
    "metadata = MetaData()\n",
    "train_data_label_table = Table('train_data_label', metadata, *l_columns)\n",
    "metadata.create_all(engine)\n",
    "\n",
    "# Map the class imperatively\n",
    "mapper_registry.map_imperatively(\n",
    "    class_ = Train_data_label,\n",
    "    local_table = train_data_label_table\n",
    ")\n",
    "\n",
    "# Define a class to map to the table\n",
    "class Train_data_meta:\n",
    "    pass\n",
    "\n",
    "# Create columns\n",
    "meta_column_types = [String] * len(meta_columns)\n",
    "m_columns = [Column('id', Integer, primary_key=True, autoincrement=True), \n",
    "             Column(\"train_data_id\", Integer, ForeignKey(Train_data.id))] + [\n",
    "    Column(name, type) for name, type in zip(meta_columns, meta_column_types)    \n",
    "]\n",
    "\n",
    "# Create table\n",
    "metadata = MetaData()\n",
    "train_data_meta_table = Table('train_data_meta', metadata, *m_columns)\n",
    "metadata.create_all(engine)\n",
    "\n",
    "# Map the class imperatively\n",
    "mapper_registry.map_imperatively(\n",
    "    class_ = Train_data_meta,\n",
    "    local_table = train_data_meta_table\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = get_engine()\n",
    "mapper_registry = registry()\n",
    "\n",
    "label_columns = list([\"Class\", \"Risk Level\"])\n",
    "feature_columns = list(test_gearbox.columns)\n",
    "feature_columns = [x for x in feature_columns if x not in label_columns]\n",
    "meta_columns = list(test_gearbox.select_dtypes(include=['object']).columns)\n",
    "meta_columns = [x for x in meta_columns if x not in label_columns]\n",
    "feature_columns = [x for x in feature_columns if x not in meta_columns]\n",
    "\n",
    "# Define a class to map to the table\n",
    "class test_data:\n",
    "    pass\n",
    "\n",
    "# Create columns\n",
    "feature_column_types = [Float] * len(feature_columns)\n",
    "f_columns = [Column('id', Integer, primary_key=True, autoincrement=True)] + [\n",
    "    Column(name, type) for name, type in zip(feature_columns, feature_column_types)\n",
    "]\n",
    "\n",
    "# Create table\n",
    "metadata = MetaData()\n",
    "test_data_table = Table('test_data', metadata, *f_columns)\n",
    "metadata.create_all(engine)\n",
    "\n",
    "# Map the class imperatively\n",
    "mapper_registry.map_imperatively(\n",
    "    class_ = test_data,\n",
    "    local_table = test_data_table\n",
    ")\n",
    "\n",
    "# Define a class to map to the table\n",
    "class test_data_label:\n",
    "    pass\n",
    "\n",
    "# Create columns\n",
    "label_column_types = list([Integer, String])\n",
    "l_columns = [Column('id', Integer, primary_key=True, autoincrement=True), \n",
    "             Column(\"test_data_id\", Integer, ForeignKey(test_data.id))] + [\n",
    "    Column(name, type) for name, type in zip(label_columns, label_column_types)    \n",
    "]\n",
    "\n",
    "# Create table\n",
    "metadata = MetaData()\n",
    "test_data_label_table = Table('test_data_label', metadata, *l_columns)\n",
    "metadata.create_all(engine)\n",
    "\n",
    "# Map the class imperatively\n",
    "mapper_registry.map_imperatively(\n",
    "    class_ = test_data_label,\n",
    "    local_table = test_data_label_table\n",
    ")\n",
    "\n",
    "# Define a class to map to the table\n",
    "class test_data_meta:\n",
    "    pass\n",
    "\n",
    "# Create columns\n",
    "meta_column_types = [String] * len(meta_columns)\n",
    "m_columns = [Column('id', Integer, primary_key=True, autoincrement=True), \n",
    "             Column(\"test_data_id\", Integer, ForeignKey(test_data.id))] + [\n",
    "    Column(name, type) for name, type in zip(meta_columns, meta_column_types)    \n",
    "]\n",
    "\n",
    "# Create table\n",
    "metadata = MetaData()\n",
    "test_data_meta_table = Table('test_data_meta', metadata, *m_columns)\n",
    "metadata.create_all(engine)\n",
    "\n",
    "# Map the class imperatively\n",
    "mapper_registry.map_imperatively(\n",
    "    class_ = test_data_meta,\n",
    "    local_table = test_data_meta_table\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session(bind=engine)\n",
    "print(session.query(Train_data).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Session\n",
    "session = Session(bind=engine)\n",
    "#Create objects to insert\n",
    "train_data_list, train_data_label_list, train_data_meta_list, test_data_list, test_data_label_list, test_data_meta_list = list(),list(),list(),list(),list(),list()\n",
    "\n",
    "train_count = session.query(Train_data).count()\n",
    "for index, row in train_gearbox.iterrows():\n",
    "    train_data_piece = Train_data()\n",
    "    train_data_label_piece = Train_data_label()\n",
    "    train_data_meta_piece = Train_data_meta()\n",
    "\n",
    "    train_data_label_piece.train_data_id = 0 + index + 1\n",
    "    train_data_meta_piece.train_data_id = 0 + index + 1\n",
    "\n",
    "    for column in train_gearbox.columns:\n",
    "        if(column in label_columns):\n",
    "            setattr(train_data_label_piece, column, row[column])\n",
    "        elif(column in meta_columns):\n",
    "            setattr(train_data_meta_piece, column, row[column])\n",
    "        else:\n",
    "            setattr(train_data_piece, column, row[column])\n",
    "\n",
    "    train_data_list.append(train_data_piece)\n",
    "    train_data_label_list.append(train_data_label_piece)\n",
    "    train_data_meta_list.append(train_data_meta_piece)\n",
    "\n",
    "test_count = session.query(Train_data).count()\n",
    "for index, row in test_gearbox.iterrows():\n",
    "    test_data_piece = test_data()\n",
    "    test_data_label_piece = test_data_label()\n",
    "    test_data_meta_piece = test_data_meta()\n",
    "\n",
    "    test_data_label_piece.test_data_id = 0 + index + 1\n",
    "    test_data_meta_piece.test_data_id = 0 + index + 1\n",
    "\n",
    "    for column in test_gearbox.columns:\n",
    "        if(column in label_columns):\n",
    "            setattr(test_data_label_piece, column, row[column])\n",
    "        elif(column in meta_columns):\n",
    "            setattr(test_data_meta_piece, column, row[column])\n",
    "        else:\n",
    "            setattr(test_data_piece, column, row[column])\n",
    "\n",
    "\n",
    "    test_data_list.append(test_data_piece)\n",
    "    test_data_label_list.append(test_data_label_piece)\n",
    "    test_data_meta_list.append(test_data_meta_piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commit Objects\n",
    "session.add_all(train_data_list)\n",
    "session.add_all(train_data_label_list)\n",
    "session.add_all(train_data_meta_list)\n",
    "session.add_all(test_data_list)\n",
    "session.add_all(test_data_label_list)\n",
    "session.add_all(test_data_meta_list)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQLAlchemy Setup\n",
    "engine = get_engine()\n",
    "metadata = MetaData()\n",
    "metadata.reflect(bind=engine)\n",
    "Base = automap_base(metadata=metadata)\n",
    "Base.prepare(autoload_with=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature(Base):\n",
    "    __tablename__ = 'feature'\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String)\n",
    "    description = Column(String)\n",
    "\n",
    "class Feature_statistic(Base):\n",
    "    __tablename__ = 'feature_statistic'\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    Column(\"feature_id\", Integer, ForeignKey(Feature.id))\n",
    "    mean = Column(Integer)\n",
    "    median = Column(Integer)\n",
    "\n",
    "class Label(Base):\n",
    "    __tablename__ = 'label'\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String)\n",
    "    description = Column(String)\n",
    "\n",
    "class Label_statistic(Base):\n",
    "    __tablename__ = 'label_statistic'\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    Column(\"label_id\", Integer, ForeignKey(Label.id))\n",
    "    mean = Column(Integer)\n",
    "    median = Column(Integer)\n",
    "\n",
    "metadata.create_all(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = get_engine()\n",
    "metadata = MetaData()\n",
    "metadata.reflect(bind=engine)\n",
    "Base = automap_base(metadata=metadata)\n",
    "Base.prepare(autoload_with=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Trainingsdata\n",
    "def loadData(tableName):\n",
    "    # Get the train_data table\n",
    "    table = Table(tableName, metadata)\n",
    "    session = Session(bind=engine)\n",
    "    batch_size = 500\n",
    "    query = session.query(table)\n",
    "    #Load Query into Dataframe\n",
    "    df = pd.DataFrame()\n",
    "    for batch in pd.read_sql_query(query.statement, engine, chunksize=batch_size):\n",
    "        df = pd.concat([df, batch], ignore_index=True)\n",
    "        excluded_columns=[\"id\"]\n",
    "        if excluded_columns:\n",
    "            df = df.drop(columns=excluded_columns)\n",
    "    return df\n",
    "\n",
    "train_X = loadData(\"train_data\")\n",
    "train_y = loadData(\"train_data_label\").drop(columns=[\"train_data_id\", \"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_object(Base, class_name, **kwargs):\n",
    "    mapped_class = Base.classes[class_name]\n",
    "    # Create an instance of the mapped class\n",
    "    obj = mapped_class()\n",
    "    # Set the object's attributes\n",
    "    for key, value in kwargs.items():\n",
    "        setattr(obj, key, value)\n",
    "    return obj\n",
    "\n",
    "\n",
    "session = Session(engine)\n",
    "for i, feature in enumerate(train_X.columns):\n",
    "    featureObj = create_object(Base, \"feature\", id = i + 1,  name = feature, description = \"to be added manually\")\n",
    "    feature_statisticObj = create_object(Base, \"feature_statistic\", id = i + 1, feature_id = i + 1, mean = train_X[feature].mean(), median = train_X[feature].median())\n",
    "    session.add(featureObj)\n",
    "    session.add(feature_statisticObj)\n",
    "    session.commit()\n",
    "\n",
    "session = Session(engine)\n",
    "for i, label in enumerate(train_y.columns):\n",
    "    labelObj = create_object(Base, \"label\", id = i + 1,  name = label, description = \"to be added manually\")\n",
    "    feature_statisticObj = create_object(Base, \"label_statistic\", id = i + 1, feature_id = i + 1, mean = train_X[feature].mean(), median = train_X[feature].median())\n",
    "    session.add(labelObj)\n",
    "    session.add(feature_statisticObj)\n",
    "    session.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a561ff09736c45ea8c4c6a2f2879bc77e5e64e20329dfa0346a65d0599b7f690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
