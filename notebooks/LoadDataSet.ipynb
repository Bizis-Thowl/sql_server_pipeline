{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "c3d05f7d-fb65-4e9f-a566-814f74338ed5",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import math\n",
                "from datetime import datetime, timedelta, timezone\n",
                "\n",
                "import pandas as pd\n",
                "from sqlalchemy import MetaData\n",
                "from sqlalchemy.ext.automap import automap_base\n",
                "from sqlalchemy.orm import Session, registry\n",
                "\n",
                "from NextVisionML.util import get_engine"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "531a76dc-cb5a-4556-95e6-f783ece904b2",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "#SQLAlchemy Setup\n",
                "engine = get_engine()\n",
                "metadata = MetaData()\n",
                "metadata.reflect(bind=engine)\n",
                "Base = automap_base(metadata=metadata)\n",
                "Base.prepare(autoload_with=engine)\n",
                "mapper_registry = registry()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "8079c9ac-b15f-452d-95a3-d860e2b17c49",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "#Loading Data\n",
                "failures_2016 = pd.read_csv(\"../data/init/failures-2016.csv\", sep=\";\")\n",
                "failures_2017 = pd.read_csv(\"../data/init/failures-2017.csv\", sep=\";\")\n",
                "metmast_2016 = pd.read_csv(\"../data//init/metmast-2016.csv\", sep=\";\")\n",
                "metmast_2017 = pd.read_csv(\"../data//init/metmast-2017.csv\", sep=\";\")\n",
                "signals_2016 = pd.read_csv(\"../data//init/signals-2016.csv\", sep=\";\")\n",
                "signals_2017 = pd.read_csv(\"../data//init/signals-2017.csv\", sep=\";\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "f7981c11-f86f-4d5d-878b-c7bbeef195b4",
                "language": "python",
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Signale beider Jahre kombinieren\n",
                "signals = pd.concat([signals_2016, signals_2017])\n",
                "\n",
                "turbine_names = signals[\"Turbine_ID\"].unique()\n",
                "\n",
                "def create_df_for_each_turbine(signals):\n",
                "    turbine_dfs = list()\n",
                "\n",
                "    for turbine in turbine_names:\n",
                "        turbine_df = signals[signals[\"Turbine_ID\"] == turbine]\n",
                "        turbine_df = turbine_df.sort_values(\"Timestamp\")\n",
                "        turbine_df = turbine_df.reset_index(drop=True)\n",
                "        turbine_dfs.append(turbine_df)\n",
                "\n",
                "    return turbine_dfs\n",
                "\n",
                "turbine_dfs = create_df_for_each_turbine(signals)\n",
                "\n",
                "#ZusammenfÃ¼hren und sortieren\n",
                "metmast = pd.concat([metmast_2016, metmast_2017])\n",
                "metmast = metmast.sort_values(\"Timestamp\")\n",
                "\n",
                "# drop broken met data\n",
                "metmast = metmast.drop([\"Min_Winddirection2\", \"Max_Winddirection2\", \"Avg_Winddirection2\", \"Var_Winddirection2\"], axis=1)\n",
                "\n",
                "# Fill met data\n",
                "metmast = metmast.fillna(method = \"ffill\")\n",
                "metmast = metmast.fillna(method = \"bfill\")\n",
                "metmast.isna().sum().sum()\n",
                "\n",
                "failures = pd.concat([failures_2016, failures_2017])\n",
                "\n",
                "#Mergen\n",
                "def JoinMetamast(df:pd.DataFrame):\n",
                "    df = df.ffill()\n",
                "    df = df.bfill()\n",
                "    df = pd.merge(df, metmast, on=\"Timestamp\", how=\"left\")\n",
                "    df = df.ffill()\n",
                "    df = df.bfill()\n",
                "    df.isna().sum().sum()\n",
                "    return df\n",
                "\n",
                "merged = list()\n",
                "for turbine_df in turbine_dfs:\n",
                "    merged.append(JoinMetamast(turbine_df))\n",
                "merged_df = pd.concat(merged)\n",
                "\n",
                "l = list()\n",
                "l.append(failures[failures[\"Component\"] == \"GENERATOR_BEARING\"])\n",
                "l.append(failures[failures[\"Component\"] == \"GENERATOR\"])\n",
                "\n",
                "failures_gearbox = pd.concat(l)\n",
                "\n",
                "failures_gearbox.reset_index(drop=True, inplace=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "b98ad47e-87dd-433f-92e9-5dca69638266",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "#Util Functions\n",
                "def get_round_minute_diff(datetime_in: datetime) -> timedelta:\n",
                "    min = datetime_in.minute\n",
                "    rounded_min = round(min, -1)\n",
                "    diff = rounded_min - min\n",
                "    return timedelta(minutes=diff)\n",
                "\n",
                "def convert_round_minute_to_time(datetime_in: datetime) -> datetime:\n",
                "    td = get_round_minute_diff(datetime_in)\n",
                "    return datetime_in + td"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def insert_error(row, failure_turbine, failure_ts:datetime, days_per_class, target_name, risk_levels, data_result):\n",
                "    data_ts = str(row[\"Timestamp\"])\n",
                "    row_ = row.copy()\n",
                "    data_ts = datetime.fromisoformat(data_ts)\n",
                "    time_before_failure = failure_ts - data_ts         \n",
                "    if(row[target_name]==\"empty\" and time_before_failure>timedelta(minutes=1)*-1 and str(row[\"Turbine_ID\"]) == failure_turbine and time_before_failure <= timedelta(days=days_per_class*5)):             \n",
                "        if(time_before_failure>timedelta(days=days_per_class*4)):            \n",
                "            row_[target_name] = risk_levels[0]            \n",
                "        elif(time_before_failure>timedelta(days=days_per_class*3)):\n",
                "            row_[target_name] = risk_levels[1]\n",
                "        elif(time_before_failure>timedelta(days=days_per_class*2)):\n",
                "            row_[target_name] = risk_levels[2]\n",
                "        elif(time_before_failure>timedelta(days=days_per_class*1)):\n",
                "            row_[target_name] = risk_levels[3]\n",
                "        elif(time_before_failure<timedelta(days=days_per_class*1) and time_before_failure>timedelta(minutes=1)*-1):\n",
                "            row_[target_name] = risk_levels[4]\n",
                "    data_result.append(row_)\n",
                "    \n",
                "failures_gearbox = failures_gearbox.sort_values(by='Timestamp').reset_index(drop=True)\n",
                "data = merged_df.copy()\n",
                "data = data.sort_values(by='Timestamp').reset_index(drop=True)\n",
                "\n",
                "target_name = \"Risk Level\"\n",
                "risk_levels = [\"low\", \"low-med\", \"medium\", \"med-high\", \"high\",]\n",
                "days_per_class = 9\n",
                "data[target_name] = \"empty\" \n",
                "\n",
                "for index, row in failures_gearbox.iterrows():\n",
                "    data_result = list()\n",
                "    failure_turbine = str(row[\"Turbine_ID\"])\n",
                "    failure_ts = str(row[\"Timestamp\"])\n",
                "    failure_ts = datetime.fromisoformat(failure_ts)\n",
                "    failure_ts = convert_round_minute_to_time(failure_ts)\n",
                "    #data.apply(insert_error, args=(failure_turbine, failure_ts, days_per_class, target_name, risk_levels, data_result), axis=1)\n",
                "    for index, row_ in data.iterrows():\n",
                "        data_ts = str(row_[\"Timestamp\"])\n",
                "        data_ts = datetime.fromisoformat(data_ts)\n",
                "        time_before_failure = failure_ts - data_ts         \n",
                "        if(row_[target_name]==\"empty\" and time_before_failure>timedelta(minutes=1)*-1 and str(row_[\"Turbine_ID\"]) == failure_turbine and time_before_failure <= timedelta(days=days_per_class*5)):             \n",
                "            if(time_before_failure>timedelta(days=days_per_class*4)):            \n",
                "                row_[target_name] = risk_levels[0]            \n",
                "            elif(time_before_failure>timedelta(days=days_per_class*3)):\n",
                "                row_[target_name] = risk_levels[1]\n",
                "            elif(time_before_failure>timedelta(days=days_per_class*2)):\n",
                "                row_[target_name] = risk_levels[2]\n",
                "            elif(time_before_failure>timedelta(days=days_per_class*1)):\n",
                "                row_[target_name] = risk_levels[3]\n",
                "            elif(time_before_failure<timedelta(days=days_per_class*1) and time_before_failure>timedelta(minutes=1)*-1):\n",
                "                row_[target_name] = risk_levels[4]\n",
                "        data_result.append(row_)\n",
                "    data = pd.DataFrame(data_result)\n",
                "    data = data.sort_values(by='Timestamp').reset_index(drop=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data[target_name] = data[target_name].apply(lambda x: 'low' if x == 'empty' else x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data[target_name].describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "labeled_df  = data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "labeled_df[target_name].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "37849cb2-7c0a-4aff-be0c-9eeec8da6f85",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "labeled_df[\"Turbine_ID\"].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "failures_gearbox[failures_gearbox[\"Turbine_ID\"] == \"T06\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "filtered_df = labeled_df.copy()\n",
                "filtered_df[filtered_df[target_name] == risk_levels[0]] = 0\n",
                "filtered_df[filtered_df[target_name] == risk_levels[1]] = 1\n",
                "filtered_df[filtered_df[target_name] == risk_levels[2]] = 2\n",
                "filtered_df[filtered_df[target_name] == risk_levels[3]] = 3\n",
                "filtered_df[filtered_df[target_name] == risk_levels[4]] = 4\n",
                "filtered_df[target_name].plot()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "a4918270-51b4-47b8-90d5-4f44746b2da9",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "#Alle Daten ab August 2017 liegen im Testset\n",
                "split_criterion_reg = labeled_df[\"Timestamp\"] >= \"2017-07-00T00:00:00+00:00\"\n",
                "\n",
                "test_gearbox = labeled_df[split_criterion_reg].reset_index(drop=True)#.iloc[:100].reset_index(drop=True)\n",
                "train_gearbox = labeled_df[~split_criterion_reg].reset_index(drop=True)#.iloc[:100].reset_index(drop=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "filtered_df = test_gearbox.copy()\n",
                "filtered_df[filtered_df[target_name] == risk_levels[0]] = 0\n",
                "filtered_df[filtered_df[target_name] == risk_levels[1]] = 1\n",
                "filtered_df[filtered_df[target_name] == risk_levels[2]] = 2\n",
                "filtered_df[filtered_df[target_name] == risk_levels[3]] = 3\n",
                "filtered_df[filtered_df[target_name] == risk_levels[4]] = 4\n",
                "filtered_df[target_name].plot()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "4b19cd84-21a7-4bcb-93e4-f3b25eb765ca",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "print(train_gearbox.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "85ae35e6-d62d-4584-8137-80c4bb8b2b51",
                "language": "python",
                "tags": []
            },
            "outputs": [],
            "source": [
                "from utils.create_object import create_object\n",
                "from utils.create_objects import create_objects"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "8cc317eb-b006-40fe-ac77-fdcc1f061711",
                "language": "python",
                "tags": []
            },
            "outputs": [],
            "source": [
                "context = dict()\n",
                "context[\"base\"] = Base\n",
                "context[\"session\"] = Session(bind=engine)\n",
                "\n",
                "data_meta = create_object(context, \"data_meta\", with_commit=True)\n",
                "\n",
                "groupings = [\"train\", \"test\"]\n",
                "datapoint_mappings = {}\n",
                "for i, grouping_val in enumerate(groupings):\n",
                "    datapoint_mappings[grouping_val] = create_object(context, \"datapoint_mappings\", with_commit=True,\n",
                "        data_meta_id = 1,\n",
                "        grouping = grouping_val\n",
                "    )\n",
                "\n",
                "label_names = [\"Risk Level\"]\n",
                "label_entries = {}\n",
                "for i, label in enumerate(label_names):\n",
                "    label_entries[label] = create_object(context, \"label\", with_commit=True,\n",
                "        data_meta_id = 1,\n",
                "        name = label,\n",
                "        description = \"TBD\"\n",
                "    )\n",
                "    \n",
                "label_values = {}\n",
                "for i, label in enumerate(label_names):\n",
                "    label_values[label] = {}\n",
                "    label_values_ = train_gearbox[label].unique()\n",
                "    for j, label_value in enumerate(label_values_):\n",
                "        label_values[label][label_value] = create_object(context, \"label_categorical\", with_commit=True,\n",
                "            label_id = label_entries[label].id,\n",
                "            category = label_value,\n",
                "            description = \"TBD\"\n",
                "        )\n",
                "\n",
                "time_name = \"Timestamp\"\n",
                "filter_names = [\"Turbine_ID\"]\n",
                "meta_info_names = filter_names + [time_name]\n",
                "feature_names = [feature for feature in train_gearbox.columns if feature not in label_names and feature not in meta_info_names]\n",
                "aggregated_meta_feature_list = meta_info_names + feature_names\n",
                "aggregated_meta_feature_list = [feature for feature in aggregated_meta_feature_list]\n",
                "feature_entries = {}\n",
                "filter_entries = {}\n",
                "for i, name in enumerate(aggregated_meta_feature_list):\n",
                "    type_ = \"data\"\n",
                "    if name==time_name:\n",
                "        time = create_object(context, \"time\", with_commit=True,\n",
                "            data_meta_id = 1,\n",
                "            name = name,\n",
                "            description = \"TBD\"\n",
                "        )\n",
                "    elif name in filter_names:\n",
                "        filter = create_object(context, \"filter\", with_commit=True,\n",
                "            data_meta_id = 1,\n",
                "            name = name,\n",
                "            description = \"TBD\"\n",
                "        )\n",
                "        filter_entries[name] = filter\n",
                "    else:\n",
                "        feature = create_object(context, \"feature\", with_commit=True,\n",
                "            data_meta_id = 1,\n",
                "            name = name,\n",
                "            type_ = type_,\n",
                "            description = \"TBD\"\n",
                "        )\n",
                "        feature_entries[name] = feature\n",
                "\n",
                "def process_row(row, context, datapoint_id, mapping_id, feature_entries, filter_entries):\n",
                "    create_object(context, \"datapoint\", with_commit = True,\n",
                "                              id=datapoint_id,\n",
                "                              datapoint_mappings_id=mapping_id,\n",
                "                              datetime=datetime.strptime(row[time_name][:19], \"%Y-%m-%dT%H:%M:%S\"))\n",
                "\n",
                "    filter_values = [{'datapoint_id': datapoint_id, 'filter_id': filter_entries[key].id, 'value': value} \n",
                "                    for key, value in row.items() if key in filter_entries.keys()]\n",
                "\n",
                "    # Process features\n",
                "    feature_values = [{'datapoint_id': datapoint_id, 'feature_id': feature_entries[key].id, 'value': float(value)} \n",
                "                      for key, value in row.items() if key in feature_entries.keys()]\n",
                "    \n",
                "    create_object(context, \"datapoint_class_label\", with_commit=True, datapoint_id=datapoint_id, label_id=label_entries[label_names[0]].id, value=row[label_names[0]])\n",
                "\n",
                "    # Batch create datapoint_feature_value objects\n",
                "    create_objects(context, \"datapoint_feature_value\",feature_values)\n",
                "\n",
                "    # # Create datapoint_filter object\n",
                "    create_objects(context, \"datapoint_filter\", filter_values)\n",
                "\n",
                "def apply_to_row(row, args):\n",
                "    mapping_id = int(args)\n",
                "    # base datapoint ID on row id and the mapping id and make sure datapoint_ids are not overlapping with big integer\n",
                "    datapoint_id = (mapping_id-1) * 1000000 + int(row.name) + 1\n",
                "    process_row(row, context, datapoint_id, mapping_id, feature_entries, filter_entries)\n",
                "    # commit every 5000 rows to prevent memory overflow\n",
                "    if (int(row.name) % 5000 == 0 and int(row.name) != 0):\n",
                "        context[\"session\"].commit()\n",
                "# Main loop\n",
                "for grouping, dataset in {\"test\": test_gearbox, \"train\": train_gearbox}.items():\n",
                "    # Apply the function to each row\n",
                "    dataset.apply(apply_to_row, args=(datapoint_mappings[grouping].id,), axis=1)\n",
                "\n",
                "# for all remaining datapoints\n",
                "context[\"session\"].commit()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
