{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "azdata_cell_guid": "c3d05f7d-fb65-4e9f-a566-814f74338ed5",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "from NextVisionML import MLContext\n",
                "from NextVisionML import VarianceFilter, DecisionTreeClfr, OneHotClfr"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Create an context (Loads data)\n",
                "mlcontext = MLContext()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#add feature selection methods\n",
                "mlcontext.hooks.append(VarianceFilter(mlcontext)) \n",
                "#Add data prepocessing methods\n",
                "mlcontext.train_methods.append(DecisionTreeClfr(mlcontext))\n",
                "mlcontext.train_methods.append(OneHotClfr(mlcontext))\n",
                "#select a model type\n",
                "\n",
                "#start the training process\n",
                "mlcontext.start_train_process()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import dice_ml\n",
                "from dice_ml.utils import helpers # helper functions\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import hyperopt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from NextVisionML import load_context\n",
                "\n",
                "sqlContext, train_with_labels, test_with_labels = load_context()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_with_labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dataset = pd.concat([mlcontext.iter_train_X[2], mlcontext.iter_train_y[2]], axis = 0)\n",
                "cols = []#[\"Amb_WindSpeed_Std\", \"Avg_Precipitation\", \"Grd_Prod_CosPhi_Avg\", \"Max_Precipitation\", \"Min_Precipitation\"]\n",
                "train_dataset = train_dataset.drop(columns=cols)\n",
                "d = dice_ml.Data(\n",
                "    dataframe=train_dataset,\n",
                "    continuous_features= list(train_dataset.drop(columns=[\"Risk Level\"]).columns),\n",
                "    outcome_name=\"Risk Level\")\n",
                "\n",
                "# Using sklearn backend\n",
                "m = dice_ml.Model(model=mlcontext.iter_objs[2][\"model\"], backend=\"sklearn\")\n",
                "# Using method=random for generating CFs\n",
                "exp = dice_ml.Dice(d, m, method=\"random\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_data = mlcontext.iter_test_X[2].drop(columns = cols)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "e1 = exp.generate_counterfactuals(test_data.iloc[:5].copy(), total_CFs=2, desired_class=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "e1.visualize_as_dataframe(show_only_changes=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from sklearn.metrics import balanced_accuracy_score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleNetSoftmax(torch.nn.Module):\n",
                "    def __init__(self, input_size, output_size, layer_count):\n",
                "        super(SimpleNetSoftmax, self).__init__()\n",
                "        self.call_super_init\n",
                "        self.fc = torch.nn.ModuleList()\n",
                "        self.lc = layer_count\n",
                "        for _ in range(layer_count-1):\n",
                "            self.fc.append(torch.nn.Linear(input_size, input_size*2))\n",
                "            input_size*=2\n",
                "        self.fc.append(torch.nn.Linear(input_size, output_size))\n",
                "        #self.sigmoid = torch.sigmoid\n",
                "\n",
                "    def forward(self, X):\n",
                "        y = X\n",
                "        for layer in self.fc:\n",
                "            y = layer(y)\n",
                "        y = torch.softmax(y, dim=1)\n",
                "        return y\n",
                "    \n",
                "i=0\n",
                "train_X = torch.tensor(mlcontext.iter_train_X[i].astype('float32').values)\n",
                "min, _ = train_X.min(dim=0)\n",
                "max, _ = train_X.max(dim=0)\n",
                "normalized_X = (train_X - min) / (max-min + 1)\n",
                "\n",
                "normalized_X = normalized_X\n",
                "\n",
                "train_y = torch.tensor(mlcontext.iter_objs[i][\"integer_encoded_train_y\"])\n",
                "random_seed = 5521\n",
                "input_size = len(mlcontext.iter_train_X[i].columns)\n",
                "num_classes = 5\n",
                "layer_count = 4\n",
                "num_epochs = 5  \n",
                "one_hot_encoded = torch.nn.functional.one_hot(train_y.to(torch.int64), num_classes).float()\n",
                "torch.manual_seed(random_seed)\n",
                "net = SimpleNetSoftmax(input_size, num_classes, layer_count)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.SGD(net.parameters(), lr=0.005)\n",
                "\n",
                "#Training loop\n",
                "for epoch in range(num_epochs):\n",
                "    outputs = net(normalized_X)\n",
                "    loss = criterion(outputs, one_hot_encoded)\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "#Evluation     \n",
                "test_X = torch.tensor(mlcontext.iter_test_X[i].astype('float32').values)\n",
                "min, _ = test_X.min(dim=0)\n",
                "max, _ = test_X.max(dim=0)\n",
                "test_X_n = (test_X - min) / (max-min + 1)\n",
                "\n",
                "test_X_n = test_X_n\n",
                "\n",
                "pred = net(test_X_n)\n",
                "_ , pred = torch.max(pred, 1) #Reverse One Hot\n",
                "#pred_mem = pred.tolist()\n",
                "import pandas as pd\n",
                "from alibi.confidence import TrustScore\n",
                "import numpy as np\n",
                "def get_trust_scores(train_X, train_y, test_X, test_pred):  \n",
                "    #test_pred = pd.DataFrame(test_pred)  \n",
                "    #class_mapping = {\"low\": 0, \"low-med\": 1, \"medium\": 2, \"med-high\": 3, \"high\": 4}\n",
                "    #train_y= train_y[\"Risk Level\"].map(class_mapping)\n",
                "    #test_pred = test_pred[0].map(class_mapping)\n",
                "    ts = TrustScore()\n",
                "    ts.fit(train_X, train_y, classes=5)\n",
                "    #classes = {\"low\", \"low-med\", \"medium\", \"med-high\", \"high\"}\n",
                "    score, closest_class = ts.score(test_X, test_pred, k=2)\n",
                "    return score\n",
                "\n",
                "\n",
                "#df_ = pd.DataFrame(train_y.numpy().reshape(-1,1))\n",
                "#df_ = df_.rename(columns={0: \"Risk Level\"})\n",
                "t = get_trust_scores(normalized_X.numpy(), pd.DataFrame(train_y.numpy().reshape(-1,1).astype(dtype = np.int64))[0], test_X_n.numpy(), pred.numpy().reshape(-1,1))\n",
                "print(t)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(t)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from alibi.confidence import TrustScore\n",
                "def get_trust_scores(train_X, train_y, test_X, test_pred):    \n",
                "    ts = TrustScore()\n",
                "    ts.fit(train_X, train_y, classes=5)\n",
                "    #classes = {\"low\", \"low-med\", \"medium\", \"med-high\", \"high\"}\n",
                "    score, closest_class = ts.score(test_X, test_pred, k=2)\n",
                "    return score\n",
                "ts = get_trust_scores(train_X.numpy(), train_y.numpy(), test_X_n.numpy(), pred)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "i=0\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "label_encoder = LabelEncoder()\n",
                "integer_encoded_train_y = label_encoder.fit_transform(mlcontext.iter_train_y[i].values.ravel())\n",
                "integer_encoded_test = label_encoder.transform(mlcontext.iter_test_y[i].values.ravel())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "integer_encoded_test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mlcontext.iter_train_y[0].columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from NextVisionML.util import update_object_attributes\n",
                "from alibi.confidence import TrustScore\n",
                "from sklearn.metrics import balanced_accuracy_score\n",
                "import pandas as pd\n",
                "\n",
                "def get_trust_scores(train_X, train_y, test_X, test_pred):  \n",
                "    test_pred = pd.DataFrame(test_pred)  \n",
                "    class_mapping = {\"low\": 0, \"low-med\": 1, \"medium\": 2, \"med-high\": 3, \"high\": 4}\n",
                "    train_y= train_y[\"Risk Level\"].map(class_mapping)\n",
                "    test_pred = test_pred[0].map(class_mapping)\n",
                "    ts = TrustScore()\n",
                "    ts.fit(train_X, train_y, classes=5)\n",
                "    #classes = {\"low\", \"low-med\", \"medium\", \"med-high\", \"high\"}\n",
                "    score, closest_class = ts.score(test_X, test_pred, k=2)\n",
                "    return score\n",
                "i=0\n",
                "dtc = DecisionTreeClassifier(\n",
                "    max_depth = 3,\n",
                "    min_samples_leaf = 2,\n",
                "    random_state = 5,\n",
                "    max_features = 3\n",
                ")\n",
                "dtc.fit(mlcontext.iter_train_X[i].values, mlcontext.iter_train_y[i].values)\n",
                "#self.mlContext.iter_objs[i][\"model\"][\"dtc\"] = dtc\n",
                "eval_predict = dtc.predict(mlcontext.iter_test_X[i].values) \n",
                "balanced_accuracy = balanced_accuracy_score(mlcontext.iter_test_y[i], eval_predict) \n",
                "\n",
                "mlcontext.iter_objs[i][\"dtc_pred\"] = eval_predict\n",
                "mlcontext.iter_objs[i][\"dtc_balanced_accuracy\"] = balanced_accuracy\n",
                "mlcontext.iter_objs[i][\"dtc_trust_scores\"] = get_trust_scores(mlcontext.iter_train_X[i].values, mlcontext.iter_train_y[i], mlcontext.iter_test_X[i].values, eval_predict.reshape(-1, 1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(mlcontext.iter_objs[i][\"dtc_trust_scores\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mlcontext.iter_train_y[i][\"Risk Level\"].unique()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mlcontext.iter_train_y[i].shape"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
