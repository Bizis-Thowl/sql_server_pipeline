{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "azdata_cell_guid": "c3d05f7d-fb65-4e9f-a566-814f74338ed5",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "from NextVisionML import MLContext\n",
                "from NextVisionML import VarianceFilter, DecisionTreeClfr, OneHotClfr, PcaUnsupervised"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "ename": "KeyError",
                    "evalue": "\"['Risk Level', 'Class'] not found in axis\"",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Create an context (Loads data)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mlcontext \u001b[38;5;241m=\u001b[39m \u001b[43mMLContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\repos\\sql_server_pipeline3\\notebooks\\NextVisionML\\train\\MLContext.py:31\u001b[0m, in \u001b[0;36mMLContext.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_methods \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxai_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_db_indexes \u001b[38;5;241m=\u001b[39m \u001b[43msplit_X_y_z\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_db_indexes \u001b[38;5;241m=\u001b[39m split_X_y_z(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest)\n",
                        "File \u001b[1;32mc:\\repos\\sql_server_pipeline3\\notebooks\\NextVisionML\\train\\MLContext_utils.py:4\u001b[0m, in \u001b[0;36msplit_X_y_z\u001b[1;34m(table)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_X_y_z\u001b[39m(table:pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m----> 4\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRisk Level\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mClass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatapoint_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#TODO:LoadFromDB\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(table[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRisk Level\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m     z \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(table[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatapoint_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
                        "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5433\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4782\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4782\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
                        "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:4824\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4822\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4823\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4824\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4825\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4827\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7069\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7069\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7070\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7071\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
                        "\u001b[1;31mKeyError\u001b[0m: \"['Risk Level', 'Class'] not found in axis\""
                    ]
                }
            ],
            "source": [
                "#Create an context (Loads data)\n",
                "mlcontext = MLContext()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#add feature selection methods\n",
                "mlcontext.train_preparation_methods.append(VarianceFilter(mlcontext)) \n",
                "#Add data prepocessing methods\n",
                "mlcontext.train_methods.append(DecisionTreeClfr(mlcontext))\n",
                "mlcontext.train_methods.append(PcaUnsupervised(mlcontext))\n",
                "#select a model type\n",
                "#start the training process\n",
                "mlcontext.start_train_process()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import dice_ml\n",
                "from dice_ml.utils import helpers # helper functions\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import hyperopt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from NextVisionML import load_context\n",
                "\n",
                "sqlContext, train_with_labels, test_with_labels = load_context()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_with_labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dataset = pd.concat([mlcontext.iter_train_X[2], mlcontext.iter_train_y[2]], axis = 0)\n",
                "cols = []#[\"Amb_WindSpeed_Std\", \"Avg_Precipitation\", \"Grd_Prod_CosPhi_Avg\", \"Max_Precipitation\", \"Min_Precipitation\"]\n",
                "train_dataset = train_dataset.drop(columns=cols)\n",
                "d = dice_ml.Data(\n",
                "    dataframe=train_dataset,\n",
                "    continuous_features= list(train_dataset.drop(columns=[\"Risk Level\"]).columns),\n",
                "    outcome_name=\"Risk Level\")\n",
                "\n",
                "# Using sklearn backend\n",
                "m = dice_ml.Model(model=mlcontext.iter_objs[2][\"model\"], backend=\"sklearn\")\n",
                "# Using method=random for generating CFs\n",
                "exp = dice_ml.Dice(d, m, method=\"random\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_data = mlcontext.iter_test_X[2].drop(columns = cols)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "e1 = exp.generate_counterfactuals(test_data.iloc[:5].copy(), total_CFs=2, desired_class=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "e1.visualize_as_dataframe(show_only_changes=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from sklearn.metrics import balanced_accuracy_score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleNetSoftmax(torch.nn.Module):\n",
                "    def __init__(self, input_size, output_size, layer_count):\n",
                "        super(SimpleNetSoftmax, self).__init__()\n",
                "        self.call_super_init\n",
                "        self.fc = torch.nn.ModuleList()\n",
                "        self.lc = layer_count\n",
                "        for _ in range(layer_count-1):\n",
                "            self.fc.append(torch.nn.Linear(input_size, input_size*2))\n",
                "            input_size*=2\n",
                "        self.fc.append(torch.nn.Linear(input_size, output_size))\n",
                "        #self.sigmoid = torch.sigmoid\n",
                "\n",
                "    def forward(self, X):\n",
                "        y = X\n",
                "        for layer in self.fc:\n",
                "            y = layer(y)\n",
                "        y = torch.softmax(y, dim=1)\n",
                "        return y\n",
                "    \n",
                "i=0\n",
                "train_X = torch.tensor(mlcontext.iter_train_X[i].astype('float32').values)\n",
                "min, _ = train_X.min(dim=0)\n",
                "max, _ = train_X.max(dim=0)\n",
                "normalized_X = (train_X - min) / (max-min + 1)\n",
                "\n",
                "normalized_X = normalized_X\n",
                "\n",
                "train_y = torch.tensor(mlcontext.iter_objs[i][\"integer_encoded_train_y\"])\n",
                "random_seed = 5521\n",
                "input_size = len(mlcontext.iter_train_X[i].columns)\n",
                "num_classes = 5\n",
                "layer_count = 4\n",
                "num_epochs = 5  \n",
                "one_hot_encoded = torch.nn.functional.one_hot(train_y.to(torch.int64), num_classes).float()\n",
                "torch.manual_seed(random_seed)\n",
                "net = SimpleNetSoftmax(input_size, num_classes, layer_count)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.SGD(net.parameters(), lr=0.005)\n",
                "\n",
                "#Training loop\n",
                "for epoch in range(num_epochs):\n",
                "    outputs = net(normalized_X)\n",
                "    loss = criterion(outputs, one_hot_encoded)\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "#Evluation     \n",
                "test_X = torch.tensor(mlcontext.iter_test_X[i].astype('float32').values)\n",
                "min, _ = test_X.min(dim=0)\n",
                "max, _ = test_X.max(dim=0)\n",
                "test_X_n = (test_X - min) / (max-min + 1)\n",
                "\n",
                "test_X_n = test_X_n\n",
                "\n",
                "pred = net(test_X_n)\n",
                "_ , pred = torch.max(pred, 1) #Reverse One Hot\n",
                "#pred_mem = pred.tolist()\n",
                "import pandas as pd\n",
                "from alibi.confidence import TrustScore\n",
                "import numpy as np\n",
                "def get_trust_scores(train_X, train_y, test_X, test_pred):  \n",
                "    #test_pred = pd.DataFrame(test_pred)  \n",
                "    #class_mapping = {\"low\": 0, \"low-med\": 1, \"medium\": 2, \"med-high\": 3, \"high\": 4}\n",
                "    #train_y= train_y[\"Risk Level\"].map(class_mapping)\n",
                "    #test_pred = test_pred[0].map(class_mapping)\n",
                "    ts = TrustScore()\n",
                "    ts.fit(train_X, train_y, classes=5)\n",
                "    #classes = {\"low\", \"low-med\", \"medium\", \"med-high\", \"high\"}\n",
                "    score, closest_class = ts.score(test_X, test_pred, k=2)\n",
                "    return score\n",
                "\n",
                "\n",
                "#df_ = pd.DataFrame(train_y.numpy().reshape(-1,1))\n",
                "#df_ = df_.rename(columns={0: \"Risk Level\"})\n",
                "t = get_trust_scores(normalized_X.numpy(), pd.DataFrame(train_y.numpy().reshape(-1,1).astype(dtype = np.int64))[0], test_X_n.numpy(), pred.numpy().reshape(-1,1))\n",
                "print(t)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(t)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from alibi.confidence import TrustScore\n",
                "def get_trust_scores(train_X, train_y, test_X, test_pred):    \n",
                "    ts = TrustScore()\n",
                "    ts.fit(train_X, train_y, classes=5)\n",
                "    #classes = {\"low\", \"low-med\", \"medium\", \"med-high\", \"high\"}\n",
                "    score, closest_class = ts.score(test_X, test_pred, k=2)\n",
                "    return score\n",
                "ts = get_trust_scores(train_X.numpy(), train_y.numpy(), test_X_n.numpy(), pred)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "i=0\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "label_encoder = LabelEncoder()\n",
                "integer_encoded_train_y = label_encoder.fit_transform(mlcontext.iter_train_y[i].values.ravel())\n",
                "integer_encoded_test = label_encoder.transform(mlcontext.iter_test_y[i].values.ravel())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "integer_encoded_test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mlcontext.iter_train_y[0].columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from NextVisionML.util import update_object_attributes\n",
                "from alibi.confidence import TrustScore\n",
                "from sklearn.metrics import balanced_accuracy_score\n",
                "import pandas as pd\n",
                "\n",
                "def get_trust_scores(train_X, train_y, test_X, test_pred):  \n",
                "    test_pred = pd.DataFrame(test_pred)  \n",
                "    class_mapping = {\"low\": 0, \"low-med\": 1, \"medium\": 2, \"med-high\": 3, \"high\": 4}\n",
                "    train_y= train_y[\"Risk Level\"].map(class_mapping)\n",
                "    test_pred = test_pred[0].map(class_mapping)\n",
                "    ts = TrustScore()\n",
                "    ts.fit(train_X, train_y, classes=5)\n",
                "    #classes = {\"low\", \"low-med\", \"medium\", \"med-high\", \"high\"}\n",
                "    score, closest_class = ts.score(test_X, test_pred, k=2)\n",
                "    return score\n",
                "i=0\n",
                "dtc = DecisionTreeClassifier(\n",
                "    max_depth = 3,\n",
                "    min_samples_leaf = 2,\n",
                "    random_state = 5,\n",
                "    max_features = 3\n",
                ")\n",
                "dtc.fit(mlcontext.iter_train_X[i].values, mlcontext.iter_train_y[i].values)\n",
                "#self.mlContext.iter_objs[i][\"model\"][\"dtc\"] = dtc\n",
                "eval_predict = dtc.predict(mlcontext.iter_test_X[i].values) \n",
                "balanced_accuracy = balanced_accuracy_score(mlcontext.iter_test_y[i], eval_predict) \n",
                "\n",
                "mlcontext.iter_objs[i][\"dtc_pred\"] = eval_predict\n",
                "mlcontext.iter_objs[i][\"dtc_balanced_accuracy\"] = balanced_accuracy\n",
                "mlcontext.iter_objs[i][\"dtc_trust_scores\"] = get_trust_scores(mlcontext.iter_train_X[i].values, mlcontext.iter_train_y[i], mlcontext.iter_test_X[i].values, eval_predict.reshape(-1, 1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(mlcontext.iter_objs[i][\"dtc_trust_scores\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mlcontext.iter_train_y[i][\"Risk Level\"].unique()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mlcontext.iter_train_y[i].shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sqlalchemy import MetaData, select, text\n",
                "from sqlalchemy.ext.automap import automap_base\n",
                "from sqlalchemy.orm import Session, registry\n",
                "from NextVisionML.util import get_engine \n",
                "engine = get_engine()\n",
                "metadata = MetaData()\n",
                "metadata.reflect(bind=engine)\n",
                "Base = automap_base(metadata=metadata)\n",
                "Base.prepare(autoload_with=engine)    \n",
                "mapper_registry = registry()\n",
                "\n",
                "sqlContext = dict()\n",
                "sqlContext[\"metadata\"] = metadata\n",
                "sqlContext[\"Base\"] = Base\n",
                "sqlContext[\"engine\"] = engine    \n",
                "\n",
                "#Metadata\n",
                "datapoint = metadata.tables['datapoint']\n",
                "datapoint_feature_value = metadata.tables['datapoint_feature_value']\n",
                "feature = metadata.tables['feature']\n",
                "datapoint_mappings = metadata.tables['datapoint_mappings']\n",
                "datapoint_class_label = metadata.tables[\"datapoint_class_label\"]\n",
                "datapoint_rul_label = metadata.tables[\"datapoint_rul_label\"]\n",
                "label = metadata.tables[\"label\"]    \n",
                "\n",
                "# For train set\n",
                "query_train = select(\n",
                "    datapoint.c.id.label('datapoint_id'),\n",
                "    feature.c.name.label('feature_name'),\n",
                "    datapoint_feature_value.c.value.label('feature_value')\n",
                ").select_from(\n",
                "    datapoint.join(datapoint_feature_value, datapoint.c.id == datapoint_feature_value.c.datapoint_id)\n",
                "    .join(feature, feature.c.id == datapoint_feature_value.c.feature_id)\n",
                "    .join(datapoint_mappings, datapoint.c.datapoint_mappings_id == datapoint_mappings.c.id)\n",
                ").where(\n",
                "    datapoint_mappings.c.grouping == 'train'\n",
                ")\n",
                "\n",
                "# For test set\n",
                "query_test = select(\n",
                "    datapoint.c.id.label('datapoint_id'),\n",
                "    feature.c.name.label('feature_name'),\n",
                "    datapoint_feature_value.c.value.label('feature_value')\n",
                ").select_from(\n",
                "    datapoint.join(datapoint_feature_value, datapoint.c.id == datapoint_feature_value.c.datapoint_id)\n",
                "    .join(feature, feature.c.id == datapoint_feature_value.c.feature_id)\n",
                "    .join(datapoint_mappings, datapoint.c.datapoint_mappings_id == datapoint_mappings.c.id)\n",
                ").where(\n",
                "    datapoint_mappings.c.grouping == 'test'\n",
                ")\n",
                "\n",
                "# Construct SQL query for categorical labels\n",
                "query_cat_labels = select(\n",
                "    datapoint.c.id.label('datapoint_id'),\n",
                "    label.c.name.label('label_name'),\n",
                "    datapoint_class_label.c.value.label('label_value')\n",
                ").select_from(\n",
                "    datapoint.join(datapoint_class_label, datapoint.c.id == datapoint_class_label.c.datapoint_id)\n",
                "    .join(label, label.c.id == datapoint_class_label.c.label_id)\n",
                ")\n",
                "\n",
                "# Construct SQL query for continuous labels\n",
                "query_cont_labels = select(\n",
                "    datapoint.c.id.label('datapoint_id'),\n",
                "    label.c.name.label('label_name'),\n",
                "    datapoint_rul_label.c.value.label('label_value')\n",
                ").select_from(\n",
                "    datapoint.join(datapoint_rul_label, datapoint.c.id == datapoint_rul_label.c.datapoint_id)\n",
                "    .join(label, label.c.id == datapoint_rul_label.c.label_id)\n",
                ")\n",
                "\n",
                "con = engine.connect()\n",
                "    \n",
                "    \n",
                "\n",
                "\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "select_statement = query_train\n",
                "test = list()\n",
                "with engine.connect() as conn:\n",
                "    with conn.execution_options(yield_per=10000).execute(\n",
                "        select_statement\n",
                "    ) as result:\n",
                "        for partition in result.partitions():\n",
                "            # partition is an iterable that will be at most 100 items\n",
                "            for row in partition:\n",
                "                test.append(row)\n",
                "pd.DataFrame(test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "result = con.execute(query_train)\n",
                "metadata.tables['datapoint']\n",
                "rows = result.fetchall()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sqlalchemy import MetaData, select, func\n",
                "from sqlalchemy.ext.automap import automap_base\n",
                "from sqlalchemy.orm import Session, registry\n",
                "from NextVisionML.util import get_engine \n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "from concurrent import futures\n",
                "\n",
                "engine = get_engine()\n",
                "metadata = MetaData()\n",
                "metadata.reflect(bind=engine)\n",
                "Base = automap_base(metadata=metadata)\n",
                "Base.prepare(autoload_with=engine)    \n",
                "mapper_registry = registry()\n",
                "con = engine.connect()\n",
                "\n",
                "def read_table_rows(table, offset, limit, i):\n",
                "    print(\"Start \" + str(i))\n",
                "    with Session(engine) as session:\n",
                "        # Build the SELECT statement with OFFSET and LIMIT\n",
                "        re = session.query(table).order_by(table.c.id).offset(offset).limit(limit).all()\n",
                "        print(\"End \" + str(i))\n",
                "        return re\n",
                "    \n",
                "\n",
                "def get_df_from_table(table):\n",
                "    num_threads = 48\n",
                "    table = metadata.tables[table]\n",
                "    count_statement = select(func.count().label('count')).select_from(table)\n",
                "    total_rows = con.execute(count_statement).scalar()\n",
                "    chunk_size = total_rows // num_threads\n",
                "    \n",
                "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
                "        futures_l = []\n",
                "\n",
                "        # Submit tasks for each chunk\n",
                "        for i in range(num_threads):\n",
                "            offset = i * chunk_size\n",
                "            limit = chunk_size if i < num_threads - 1 else total_rows - offset\n",
                "            future = executor.submit(read_table_rows, table, offset, limit, i)\n",
                "            futures_l.append(future)\n",
                "        # Wait for all tasks to complete\n",
                "        futures.wait(futures_l)\n",
                "    all_rows = []\n",
                "    for future in futures_l:\n",
                "        all_rows.extend(future.result())\n",
                "    return pd.DataFrame(all_rows)\n",
                "\n",
                "\n",
                "#datapoint = get_df_from_table(\"datapoint\")\n",
                "datapoint_feature_value =get_df_from_table(\"datapoint_feature_value\")\n",
                "#feature = get_df_from_table(\"feature\")\n",
                "#datapoint_mappings =get_df_from_table(\"datapoint_mappings\")#\n",
                "#datapoint_class_label =get_df_from_table(\"datapoint_class_label\")\n",
                "#datapoint_rul_label = get_df_from_table(\"datapoint_rul_label\")\n",
                "#label = get_df_from_table(\"label\")\n",
                "datapoint.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sqlalchemy import MetaData, select, func\n",
                "from sqlalchemy.ext.automap import automap_base\n",
                "from sqlalchemy.orm import Session, registry\n",
                "from NextVisionML.util import get_engine \n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "from concurrent import futures\n",
                "import copy\n",
                "\n",
                "engine = get_engine()\n",
                "metadata = MetaData()\n",
                "metadata.reflect(bind=engine)\n",
                "Base = automap_base(metadata=metadata)\n",
                "Base.prepare(autoload_with=engine)    \n",
                "mapper_registry = registry()\n",
                "con = engine.connect()\n",
                "\n",
                "def read_table_rows(table, offset, limit, i):\n",
                "    select_statement = select(table).order_by(table.c.id).offset(offset).limit(limit)\n",
                "    test = list()\n",
                "    with engine.connect() as conn:\n",
                "        with conn.execution_options(yield_per=10000, stream_results=True).execute(\n",
                "            select_statement\n",
                "        ) as result:\n",
                "            for i, partition in enumerate(result.partitions()):\n",
                "                if(i==1): \n",
                "                    print(str(offset) + \" \" + str(limit) + \" \"+ str(i), end='\\r')\n",
                "                for row in partition:\n",
                "                    test.append(row)\n",
                "    return test\n",
                "    \n",
                "\n",
                "def get_df_from_table(table):\n",
                "    num_threads = 48\n",
                "    table = metadata.tables[table]\n",
                "    count_statement = select(func.count().label('count')).select_from(table)\n",
                "    total_rows = con.execute(count_statement).scalar()\n",
                "    chunk_size = total_rows // num_threads\n",
                "    \n",
                "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
                "        futures_l = []\n",
                "\n",
                "        # Submit tasks for each chunk\n",
                "        for i in range(num_threads):\n",
                "            offset = i * chunk_size\n",
                "            limit = chunk_size if i < num_threads - 1 else total_rows - offset\n",
                "            future = executor.submit(read_table_rows, table, copy.deepcopy(offset), copy.deepcopy(limit), i)\n",
                "            futures_l.append(future)\n",
                "        # Wait for all tasks to complete\n",
                "        futures.wait(futures_l)\n",
                "    all_rows = []\n",
                "    for future in futures_l:\n",
                "        all_rows.extend(future.result())\n",
                "    return pd.DataFrame(all_rows)\n",
                "\n",
                "datapoint_feature_value =get_df_from_table(\"datapoint_feature_value\")\n",
                "datapoint_feature_value.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sqlalchemy import MetaData, select, func, text\n",
                "from sqlalchemy.ext.automap import automap_base\n",
                "from sqlalchemy.orm import Session, registry\n",
                "from NextVisionML.util import get_engine \n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "from concurrent import futures\n",
                "\n",
                "engine = get_engine()\n",
                "metadata = MetaData()\n",
                "metadata.reflect(bind=engine)\n",
                "Base = automap_base(metadata=metadata)\n",
                "Base.prepare(autoload_with=engine)    \n",
                "mapper_registry = registry()\n",
                "con = engine.connect()\n",
                "from sqlalchemy import text\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_df_from_table(table_name):    \n",
                "    test = list()\n",
                "    with engine.connect() as conn:\n",
                "        with conn.execution_options(yield_per=100000).execute(\n",
                "            text(\"SELECT * FROM \" + table_name)\n",
                "        ) as result:\n",
                "            for partition in result.partitions():\n",
                "                # partition is an iterable that will be at most 100 items\n",
                "                for row in partition:\n",
                "                    test.append(row)\n",
                "    return pd.DataFrame(test)\n",
                "\n",
                "datapoint_feature_value =get_df_from_table(\"datapoint_feature_value\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "session = Session(engine)\n",
                "\n",
                "test = session.query(metadata.tables[\"datapoint_feature_value\"]).all()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "session = Session(engine)\n",
                "table = metadata.tables[\"datapoint_feature_value\"]\n",
                "test = session.query(metadata.tables[\"datapoint_feature_value\"]).order_by(table.c.id).offset(1000).limit(10000).all()\n",
                "print(d)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "# Number of concurrent threads to use\n",
                " # Adjust this based on your system's capabilities\n",
                "\n",
                "# Determine the total number of rows in the table\n",
                "with engine.connect() as connection:\n",
                "    count_statement = select([table]).count()\n",
                "    total_rows = connection.execute(count_statement).scalar()\n",
                "\n",
                "# Calculate the chunk size for each thread\n",
                "chunk_size = total_rows // num_threads\n",
                "\n",
                "# Create a ThreadPoolExecutor\n",
                "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
                "    futures = []\n",
                "\n",
                "    # Submit tasks for each chunk\n",
                "    for i in range(num_threads):\n",
                "        offset = i * chunk_size\n",
                "        limit = chunk_size if i < num_threads - 1 else total_rows - offset\n",
                "        future = executor.submit(read_table_rows, offset, limit)\n",
                "        futures.append(future)\n",
                "\n",
                "    # Wait for all tasks to complete\n",
                "    concurrent.futures.wait(futures)\n",
                "\n",
                "# Retrieve results from completed futures\n",
                "all_rows = []\n",
                "for future in futures:\n",
                "    all_rows.extend(future.result())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_train = pd.read_sql_query(query_train, con)\n",
                "    df_test = pd.read_sql_query(query_test, con)\n",
                "\n",
                "    # Pivot tables to get features as columns, for both train and test\n",
                "    train = df_train.pivot_table(index='datapoint_id', columns='feature_name', values='feature_value').reset_index()\n",
                "    test = df_test.pivot_table(index='datapoint_id', columns='feature_name', values='feature_value').reset_index()\n",
                "    \n",
                "    # Execute the queries and load into DataFrames\n",
                "    df_cat_labels = pd.read_sql_query(query_cat_labels, con)\n",
                "    df_cont_labels = pd.read_sql_query(query_cont_labels, con)\n",
                "\n",
                "    # Pivot table for categorical labels\n",
                "    pivot_cat_labels = df_cat_labels.pivot_table(\n",
                "        index='datapoint_id',\n",
                "        columns='label_name',\n",
                "        values='label_value',\n",
                "        aggfunc='first'  # Use 'first' or 'max' for categorical data\n",
                "    ).reset_index()\n",
                "\n",
                "    # Pivot table for continuous labels\n",
                "    pivot_cont_labels = df_cont_labels.pivot_table(\n",
                "        index='datapoint_id',\n",
                "        columns='label_name',\n",
                "        values='label_value',\n",
                "        aggfunc='mean'  # Use 'mean' or another suitable function for numerical data\n",
                "    ).reset_index()\n",
                "    \n",
                "    # Merge categorical and continuous labels\n",
                "    pivot_labels = pd.merge(pivot_cat_labels, pivot_cont_labels, on='datapoint_id', how='outer')\n",
                "\n",
                "    # Merge labels with feature dataframes\n",
                "    train_with_labels = pd.merge(train, pivot_labels, on='datapoint_id', how='left')\n",
                "    test_with_labels = pd.merge(test, pivot_labels, on='datapoint_id', how='left')\n",
                "\n",
                "    return sqlContext, train_with_labels, test_with_labels\n",
                "    "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
